{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9adc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5e991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n"
     ]
    }
   ],
   "source": [
    "data_path = '/projects/multiphysics_aesp/aymanzyy/FullCSVs'\n",
    "filelist = glob.glob(data_path + '/*combined_csv*.csv')\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79ec3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_dataset_from_filelist(shell_filelist,batch_size,shard_size=1,shard_rank=0,prefectch_buffer_size=10000,shuffle_buffer=10000,num_parallel_calls=2):\n",
    "\n",
    "    # glob for the input files\n",
    "    filelist = tf.data.Dataset.from_tensor_slices(np.array(shell_filelist))\n",
    "    # shuffle and repeat at the input file level\n",
    "    filelist = filelist.shuffle(shuffle_buffer)\n",
    "    # filelist = filelist.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=10000,count=config['training']['epochs']))\n",
    "    # map to read files in parallel\n",
    "    ds = filelist.map(load_file_and_preprocess,num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # flatten the inputs across file boundaries\n",
    "    ds = ds.flat_map(lambda *x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "    # speficy batch size\n",
    "    ds = ds.batch(batch_size,drop_remainder=True)\n",
    "\n",
    "    # shard the data\n",
    "    ds = ds.shard(shard_size,shard_rank)\n",
    "\n",
    "    # how many inputs to prefetch to improve pipeline performance\n",
    "    ds = ds.prefetch(buffer_size=prefectch_buffer_size)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_file_and_preprocess(path):\n",
    "    pyf = tf.py_function(wrapped_loader,[path],(tf.float32,tf.float32))\n",
    "    return pyf\n",
    "\n",
    "def random_subsample(pv_data):\n",
    "    \n",
    "    img_size = pv_data.shape[0]\n",
    "    n_sub_imgs = 1000\n",
    "    sub_img_size = int(img_size / n_sub_imgs)\n",
    "    \n",
    "    random_sub_img = np.random.choice(1000)\n",
    "    \n",
    "    return pv_data[random_sub_img:img_size:n_sub_imgs,:]\n",
    "\n",
    "\n",
    "def wrapped_loader(path):\n",
    "    path = path.numpy().decode('utf-8')\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    mout = data[data['8'] > 0][['1','2','3']]\n",
    "    pv_data = data[data['8'] == 0][['1','2','3','5','6','7']]\n",
    "    \n",
    "    # ensure we have all the data\n",
    "    assert len(data) == len(mout) + len(pv_data)\n",
    "    \n",
    "    # convert to numpy and data type\n",
    "    mout = mout.to_numpy().astype(np.float32)\n",
    "    pv_data = pv_data.to_numpy().astype(np.float32)\n",
    "    \n",
    "    # sub sample\n",
    "    pv_data = random_subsample(pv_data)\n",
    "    \n",
    "    # convert data type and to TF tensor\n",
    "    mout = tf.convert_to_tensor(mout)\n",
    "    pv_data = tf.convert_to_tensor(pv_data)\n",
    "    \n",
    "    # add a batch dimension\n",
    "    mout = tf.expand_dims(mout,0)\n",
    "    pv_data = tf.expand_dims(pv_data,0)\n",
    "    \n",
    "    print(path,mout.shape,pv_data.shape)\n",
    "\n",
    "    # could do some preprocessing here\n",
    "    return (mout,pv_data)\n",
    "\n",
    "class PointNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(3,activation='relu');\n",
    "        self.fc2 = tf.keras.layers.Dense(64,activation='relu');\n",
    "        # self.fc3 = keras.layers.Dense(64,activation='relu');\n",
    "        self.fc_feature = tf.keras.layers.Dense(512,activation='relu');\n",
    "        self.maxpooling = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.fc4 = tf.keras.layers.Dense(128,activation='relu');\n",
    "        self.fc5 = tf.keras.layers.Dense(256,activation='relu');\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.1);\n",
    "        self.drop3 = tf.keras.layers.Dropout(0.3);\n",
    "        self.logist = tf.keras.layers.Dense(3,activation='linear');\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mout,pvout = inputs         # 3e8 * 2 (2 inputs) * 32 bits\n",
    "        ## proc mdata\n",
    "        mout = self.fc1(mout)        # 3e8 * 2 (weight & bias) * 32 bits\n",
    "        mout = self.fc2(mout)         # 64e8 * 2 * 32 -> gradients -> supporting other things that we don't know\n",
    "        # mout = self.fc3(mout) \n",
    "        mout = self.fc_feature(mout)  # 512e8 * 2 * 32\n",
    "        mout = tf.expand_dims(mout, 0) \n",
    "        mout = self.maxpooling(mout)\n",
    "\n",
    "        ## proc pvdata\n",
    "        pvout = self.fc1(pvout)      # 3e8 * 2 * 32\n",
    "        pvout = self.fc2(pvout)       # 64e8 * 2 * 32\n",
    "        # pvout = self.fc3(pvout)\n",
    "        pvout = self.fc4(pvout)       # 512e8 * 2 * 32\n",
    "        ##\n",
    "        mout = tf.broadcast_to(mout,[pvout.shape[0],mout.shape[1]])\n",
    "        out = tf.concat([pvout,mout],axis=-1)\n",
    "        # out = pvout+mout\n",
    "        # out = self.drop3(out)\n",
    "        out = self.fc5(out)           # 256e8 * 2 * 32\n",
    "        # out = self.drop1(out)\n",
    "        out = self.logist(out)        # 3e8 * 2 * 32\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "def train_step(mx,pv,y,model,optimizer,criterion,choice_num=10000):\n",
    "    print(mx.shape,pv.shape,y.shape)\n",
    "#     idx = np.arange(pv.shape[0])\n",
    "#     idx = np.random.choice(idx,choice_num,replace=False)\n",
    "#     pv = pv.numpy()[idx,:]\n",
    "#     y  =  y.numpy()[idx,:]\n",
    "    \n",
    "    print(idx.shape,pv.shape)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model([mx,pv],training=True)\n",
    "        loss = tf.reduce_mean(criterion(y, logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a239d9fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/cts1lcombined_csv_15s50.csv (1, 589691, 3) (1, 6845, 6)\n",
      "(589691, 3) (6845, 3) (6845, 3)\n",
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/00cts4lcombined_csv_15s50.csv (1, 1110251, 3) (1, 12898, 6)\n",
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/00cts10lcombined_csv_15s75.csv (1, 983273, 3) (1, 13081, 6)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m mx \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(mx)\n\u001b[1;32m     10\u001b[0m pv \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(pv)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     15\u001b[0m dur \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(mx, pv, y, model, optimizer, criterion, choice_num)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mx\u001b[38;5;241m.\u001b[39mshape,pv\u001b[38;5;241m.\u001b[39mshape,y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#     idx = np.arange(pv.shape[0])\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#     idx = np.random.choice(idx,choice_num,replace=False)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#     pv = pv.numpy()[idx,:]\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     y  =  y.numpy()[idx,:]\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43midx\u001b[49m\u001b[38;5;241m.\u001b[39mshape,pv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    122\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model([mx,pv],training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "training_ds = simple_dataset_from_filelist(filelist,1)\n",
    "\n",
    "model = PointNet()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "criterion = tf.keras.losses.MAE\n",
    "start = datetime.datetime.now()\n",
    "for step,(mx,pv) in enumerate(training_ds):\n",
    "    \n",
    "    mx = tf.squeeze(mx)\n",
    "    pv = tf.squeeze(pv)\n",
    "    \n",
    "    loss = train_step(mx,pv[:,0:3],pv[:,3:6],model,optimizer,criterion)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    dur = end - start\n",
    "    print(\"[%05d] time = %s loss = %f\" % (step,str(dur),loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7519d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2022-07-01",
   "language": "python",
   "name": "conda-2022-07-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
