{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9adc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5e991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n"
     ]
    }
   ],
   "source": [
    "data_path = '/projects/multiphysics_aesp/aymanzyy/FullCSVs'\n",
    "filelist = glob.glob(data_path + '/*combined_csv*.csv')\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79ec3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_dataset_from_filelist(shell_filelist,batch_size,shard_size=1,shard_rank=0,prefectch_buffer_size=10000,shuffle_buffer=10000,num_parallel_calls=None):\n",
    "\n",
    "    # glob for the input files\n",
    "    filelist = tf.data.Dataset.from_tensor_slices(np.array(shell_filelist))\n",
    "    # shuffle and repeat at the input file level\n",
    "    filelist = filelist.shuffle(shuffle_buffer)\n",
    "    # filelist = filelist.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=10000,count=config['training']['epochs']))\n",
    "    # map to read files in parallel\n",
    "    ds = filelist.map(load_file_and_preprocess,num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # flatten the inputs across file boundaries\n",
    "    ds = ds.flat_map(lambda *x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "    # speficy batch size\n",
    "    ds = ds.batch(batch_size,drop_remainder=True)\n",
    "\n",
    "    # shard the data\n",
    "    ds = ds.shard(shard_size,shard_rank)\n",
    "\n",
    "    # how many inputs to prefetch to improve pipeline performance\n",
    "    ds = ds.prefetch(buffer_size=prefectch_buffer_size)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_file_and_preprocess(path):\n",
    "    pyf = tf.py_function(wrapped_loader,[path],(tf.float32,tf.float32))\n",
    "    return pyf\n",
    "\n",
    "\n",
    "def wrapped_loader(path):\n",
    "    path = path.numpy().decode('utf-8')\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    mout = data[data['8'] > 0][['1','2','3']]\n",
    "    pv_data = data[data['8'] == 0][['1','2','3','5','6','7']]\n",
    "    \n",
    "    # ensure we have all the data\n",
    "    assert len(data) == len(mout) + len(pv_data)\n",
    "    \n",
    "    # convert data type and to TF tensor\n",
    "    mout = tf.convert_to_tensor(mout.to_numpy().astype(np.float32))\n",
    "    pv_data = tf.convert_to_tensor(pv_data.to_numpy().astype(np.float32))\n",
    "    \n",
    "    # add a batch dimension\n",
    "    mout = tf.expand_dims(mout,0)\n",
    "    pv_data = tf.expand_dims(pv_data,0)\n",
    "    \n",
    "    print(path,mout.shape,pv_data.shape)\n",
    "\n",
    "    # could do some preprocessing here\n",
    "    return (mout,pv_data)\n",
    "\n",
    "class PointNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(3,activation='relu');\n",
    "        self.fc2 = tf.keras.layers.Dense(64,activation='relu');\n",
    "        # self.fc3 = keras.layers.Dense(64,activation='relu');\n",
    "        self.fc_feature = tf.keras.layers.Dense(512,activation='relu');\n",
    "        self.maxpooling = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.fc4 = tf.keras.layers.Dense(128,activation='relu');\n",
    "        self.fc5 = tf.keras.layers.Dense(256,activation='relu');\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.1);\n",
    "        self.drop3 = tf.keras.layers.Dropout(0.3);\n",
    "        self.logist = tf.keras.layers.Dense(3,activation='linear');\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mout,pvout = inputs         # 3e8 * 2 (2 inputs) * 32 bits\n",
    "        ## proc mdata\n",
    "        mout = self.fc1(mout)        # 3e8 * 2 (weight & bias) * 32 bits\n",
    "        mout = self.fc2(mout)         # 64e8 * 2 * 32 -> gradients -> supporting other things that we don't know\n",
    "        # mout = self.fc3(mout) \n",
    "        mout = self.fc_feature(mout)  # 512e8 * 2 * 32\n",
    "        mout = tf.expand_dims(mout, 0) \n",
    "        mout = self.maxpooling(mout)\n",
    "\n",
    "        ## proc pvdata\n",
    "        pvout = self.fc1(pvout)      # 3e8 * 2 * 32\n",
    "        pvout = self.fc2(pvout)       # 64e8 * 2 * 32\n",
    "        # pvout = self.fc3(pvout)\n",
    "        pvout = self.fc4(pvout)       # 512e8 * 2 * 32\n",
    "        ##\n",
    "        mout = tf.broadcast_to(mout,[pvout.shape[0],mout.shape[1]])\n",
    "        out = tf.concat([pvout,mout],axis=-1)\n",
    "        # out = pvout+mout\n",
    "        # out = self.drop3(out)\n",
    "        out = self.fc5(out)           # 256e8 * 2 * 32\n",
    "        # out = self.drop1(out)\n",
    "        out = self.logist(out)        # 3e8 * 2 * 32\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "def train_step(mx,pv,y,model,optimizer,criterion,choice_num=10000):\n",
    "    print(mx.shape,pv.shape,y.shape)\n",
    "    idx = np.arange(pv.shape[0])\n",
    "    idx = np.random.choice(idx,choice_num,replace=False)\n",
    "    pv = pv.numpy()[idx,:]\n",
    "    y  =  y.numpy()[idx,:]\n",
    "    \n",
    "    print(idx.shape,pv.shape)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model([mx,pv],training=True)\n",
    "        loss = tf.reduce_mean(criterion(y, logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a239d9fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/fshcts1lcombined_csv_10s75.csv (1, 591207, 3) (1, 6881838, 6)\n",
      "(591207, 3) (6881838, 3) (6881838, 3)\n",
      "(10000,) (10000, 3)\n",
      "[00000] loss = 0.580495\n",
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/00cts32lcombined_csv_20s50.csv (1, 14334128, 3) (1, 116229801, 6)\n",
      "(14334128, 3) (116229801, 3) (116229801, 3)\n",
      "(10000,) (10000, 3)\n",
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/fsh00cts10lcombined_csv_20s50.csv (1, 987162, 3) (1, 13314670, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 13:17:05.446183: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 27.34GiB (rounded to 29356294144)requested by op MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-08-11 13:17:05.446593: W tensorflow/core/common_runtime/bfc_allocator.cc:491] **__************************________________________________________________________________________\n",
      "2022-08-11 13:17:05.446656: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:681 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[14334128,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/multiphysics_aesp/aymanzyy/FullCSVs/00cts4lcombined_csv_15s95.csv (1, 1106667, 3) (1, 12849317, 6)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"dense_2\" (type Dense).\n\nOOM when allocating tensor with shape[14334128,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]\n\nCall arguments received by layer \"dense_2\" (type Dense):\n  • inputs=tf.Tensor(shape=(14334128, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m mx \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(mx)\n\u001b[1;32m     10\u001b[0m pv \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(pv)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%05d\u001b[39;00m\u001b[38;5;124m] loss = \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (step,loss))\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(mx, pv, y, model, optimizer, criterion, choice_num)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx\u001b[38;5;241m.\u001b[39mshape,pv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 105\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(criterion(y, logits))\n\u001b[1;32m    107\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/lus/theta-fs0/software/thetagpu/conda/2022-07-01/mconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPointNet.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m mout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(mout)         \u001b[38;5;66;03m# 64e8 * 2 * 32 -> gradients -> supporting other things that we don't know\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# mout = self.fc3(mout) \u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m mout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmout\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 512e8 * 2 * 32\u001b[39;00m\n\u001b[1;32m     76\u001b[0m mout \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(mout, \u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m     77\u001b[0m mout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpooling(mout)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"dense_2\" (type Dense).\n\nOOM when allocating tensor with shape[14334128,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]\n\nCall arguments received by layer \"dense_2\" (type Dense):\n  • inputs=tf.Tensor(shape=(14334128, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "training_ds = simple_dataset_from_filelist(filelist,1)\n",
    "\n",
    "model = PointNet()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "criterion = tf.keras.losses.MAE\n",
    "\n",
    "for step,(mx,pv) in enumerate(training_ds):\n",
    "    \n",
    "    mx = tf.squeeze(mx)\n",
    "    pv = tf.squeeze(pv)\n",
    "    \n",
    "    loss = train_step(mx,pv[:,0:3],pv[:,3:6],model,optimizer,criterion)\n",
    "    \n",
    "    print(\"[%05d] loss = %f\" % (step,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7519d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2022-07-01",
   "language": "python",
   "name": "conda-2022-07-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
